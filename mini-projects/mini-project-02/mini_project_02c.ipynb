{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e09635a",
   "metadata": {},
   "source": [
    "File Name: mini_project_02c.ipynb\n",
    "\n",
    "Description: This script extracts comprehensive audio features from valve recordings, handles class imbalance with SMOTE, trains an SVM classifier to detect normal vs. abnormal valve states, evaluates the model on a test set, and predicts the state of new audio files while reporting both actual and predicted labels. It also documents the iterative process used to improve model performance from 88% to 99% accuracy.\n",
    "\n",
    "Note: Dataset not included due to GitHub size constraints.\n",
    "\n",
    "Record of Revisions (Date | Author | Change):  \n",
    "10/29/2025 | Rhys DeLoach | Initial creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c8ae7057-acbd-467c-b750-369bdb17850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "91143c49-df5b-4fc4-8935-e550a917d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Function\n",
    "def featureExtract(path):\n",
    "    y, sr = librosa.load(path)\n",
    "\n",
    "    # Features\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc_delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spec_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spec_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    spec_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    tonnetz = librosa.feature.tonnetz(y=y, sr=sr)\n",
    "\n",
    "    # Consolidating Features across Samples\n",
    "    feats = np.hstack([\n",
    "        np.mean(mfcc, axis=1),\n",
    "        np.std(mfcc, axis=1),\n",
    "        np.mean(mfcc_delta, axis=1),\n",
    "        np.std(mfcc_delta, axis=1),\n",
    "        np.mean(mfcc_delta2, axis=1),\n",
    "        np.std(mfcc_delta2, axis=1),\n",
    "        np.mean(chroma, axis=1),\n",
    "        np.std(chroma, axis=1),\n",
    "        np.mean(spec_centroid),\n",
    "        np.std(spec_centroid),\n",
    "        np.mean(spec_bandwidth),\n",
    "        np.std(spec_bandwidth),\n",
    "        np.mean(spec_rolloff),\n",
    "        np.std(spec_rolloff),\n",
    "        np.mean(spec_contrast, axis=1),\n",
    "        np.std(spec_contrast, axis=1),\n",
    "        np.mean(zcr),\n",
    "        np.std(zcr),\n",
    "        np.mean(rms),\n",
    "        np.std(rms),\n",
    "        np.mean(tonnetz, axis=1),\n",
    "        np.std(tonnetz, axis=1),\n",
    "    ])\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6fc255ef-8bab-48c7-9f5c-415742910e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Features/Labels and Create Audio Dataframe\n",
    "rootDir = 'data/valve'\n",
    "\n",
    "# Define feature columns\n",
    "feature_names = [f'mfcc{i}' for i in range(1, 14)] + [f'mfcc_std{i}' for i in range(1, 14)] + [f'mfcc_delta{i}' for i in range(1, 14)] + [f'mfcc_delta_std{i}' for i in range(1, 14)] + [f'mfcc_delta2{i}' for i in range(1, 14)] + [f'mfcc_delta2_std{i}' for i in range(1, 14)] + [f'chroma{i}' for i in range(1, 13)] + [f'chroma_std{i}' for i in range(1, 13)] + ['centroid', 'centroid_std', 'bandwidth', 'bandwidth_std', 'rolloff', 'rolloff_std'] + [f'contrast{i}' for i in range(1, 8)] + [f'contrast_std{i}' for i in range(1, 8)] + ['zeroCrossing', 'zeroCrossing_std', 'rms', 'rms_std'] + [f'tonnetz{i}' for i in range(1, 7) ]+ [f'tonnetz_std{i}' for i in range(1, 7)]\n",
    "features = pd.DataFrame(columns=feature_names)\n",
    "labels = []\n",
    "\n",
    "for dirPath, dirNames, fileNames in os.walk(rootDir): # Search root directory\n",
    "    for fileName in fileNames:\n",
    "        if os.path.splitext(fileName)[1] == '.wav': # Pull all .wav files\n",
    "            path = Path(os.path.join(dirPath, fileName))\n",
    "            \n",
    "            state = path.parts[2]\n",
    "            labels.append(state)\n",
    "\n",
    "            feats = featureExtract(path)\n",
    "\n",
    "            features.loc[len(features)] = feats\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2e0a1d99-9452-4e2b-8bf5-39cdc284ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=40)\n",
    "\n",
    "smote = SMOTE(random_state=40) # Resampling to account for class imbalance\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "svc = SVC(kernel='rbf', C=10, gamma='scale', class_weight='balanced', random_state=40) # Train support vector classifier model\n",
    "svc.fit(X_res, y_res)\n",
    "\n",
    "# Predict\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "20a50cd0-8ebe-4d1e-b8ba-53bd83f0d184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9892086330935251\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7966ce5a-0465-4a69-aa07-c0718889f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For file Valve1_000NB.wav...\n",
      "Actual State: normal\n",
      "Predicted State: normal\n",
      "\n",
      "For file Valve2_000AB.wav...\n",
      "Actual State: abnormal\n",
      "Predicted State: abnormal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate\n",
    "valDir = 'data/Valve_Data_for_Prediction'\n",
    "\n",
    "for dirPath, dirNames, fileNames in os.walk(valDir): # Search root directory\n",
    "    for fileName in fileNames:\n",
    "        if os.path.splitext(fileName)[1] == '.wav': # Pull all .wav files\n",
    "            path = Path(os.path.join(dirPath, fileName))\n",
    "            feats = pd.DataFrame([featureExtract(path)], columns=feature_names)\n",
    "            scaledFeats = scaler.transform(feats)\n",
    "            y_predVal = svc.predict(scaledFeats)\n",
    "            print(f'For file {fileName}...')\n",
    "            if fileName == 'Valve1_000NB.wav':\n",
    "                print('Actual State: normal')\n",
    "            else:\n",
    "               print('Actual State: abnormal') \n",
    "\n",
    "            print(f'Predicted State: {y_predVal[0]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6debb4-db8c-4d51-8377-587bb0019882",
   "metadata": {},
   "source": [
    "Observation: The main challenge I encountered was handling class imbalance. Initially, my model achieved 88% accuracy but was predicting “normal/normal” for my validation data. To address the imbalance, I tried several strategies: testing different models, adding more features, tuning hyperparameters, and resampling the data to reduce the imbalance. These steps increased my model performance to 99% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio",
   "language": "python",
   "name": "audio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
