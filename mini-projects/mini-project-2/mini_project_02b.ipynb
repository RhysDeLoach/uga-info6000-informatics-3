{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Name: mini_project_02b.ipynb\n",
    "\n",
    "Description: This program trains a deep Q-network (DQN) to optimize inventory management in a simulated environment, handling raw material and product stocks, cash flow, and variable demand. The agent learns an effective policy using experience replay, target networks, and reward normalization, achieving improved cash performance while mitigating challenges from stockout penalties.\n",
    "\n",
    "Record of Revisions (Date | Author | Change):  \n",
    "10/29/2025 | Rhys DeLoach | Initial creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import the required classes from the Inventory_env_class.py file that is provided\n",
    "from mini_project_02a import InventoryManagementEnv, NormalizeObservation, ReplayBuffer, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment instance - Notice we do not need to use gym.make here as I provided you the enviroment class\n",
    "env = InventoryManagementEnv(max_steps=50)\n",
    "\n",
    "# Normalize the observation space for better training performance\n",
    "env = NormalizeObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space which is continuous:\n",
      "Box(0.0, inf, (6,), float32)\n",
      "\n",
      "Action space which is discrete:\n",
      "Discrete(3)\n",
      "\n",
      "Observation space dimensions: 6\n",
      "Action space dimensions: 3\n"
     ]
    }
   ],
   "source": [
    "# Check the observation and action spaces\n",
    "print(f\"Observation space which is continuous:\\n{env.observation_space}\\n\")\n",
    "print(f\"Action space which is discrete:\\n{env.action_space}\\n\")\n",
    "\n",
    "# Check the dimensions of the observation space \n",
    "print(f\"Observation space dimensions: {env.observation_space.shape[0]}\")\n",
    "print(f\"Action space dimensions: {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1 | Action taken: 2\n",
      "Step: 1\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.76, Product Price: 20.34\n",
      "Demand: 7.34, Cash: 1000.00\n",
      "\n",
      "Episode 2 | Action taken: 1\n",
      "Step: 2\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.66, Product Price: 20.37\n",
      "Demand: 8.69, Cash: 995.24\n",
      "\n",
      "Episode 3 | Action taken: 1\n",
      "Step: 3\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.65, Product Price: 20.58\n",
      "Demand: 8.71, Cash: 990.58\n",
      "\n",
      "Episode 4 | Action taken: 1\n",
      "Step: 4\n",
      "Raw Inventory: 3.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.34, Product Price: 20.51\n",
      "Demand: 7.15, Cash: 985.92\n",
      "\n",
      "Episode 5 | Action taken: 0\n",
      "Step: 5\n",
      "Raw Inventory: 3.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.48, Product Price: 20.18\n",
      "Demand: 12.68, Cash: 985.92\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment to a start state\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Here we are just taking some random actions to see how the env works and what information rendering give us\n",
    "# We do this over 5 episodes\n",
    "for episode in range(5):\n",
    "    action = env.action_space.sample()  # Replace with agent policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"\\nEpisode {episode+1} | Action taken: {action}\")\n",
    "    env.render()\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>When developing the DQN solution you need a few things</u> ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU or CPU selection\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assign dimension values of state and actions\n",
    "input_dim = env.observation_space.shape[0]  # 6 dimensions\n",
    "output_dim = env.action_space.n             # 3 actions\n",
    "\n",
    "# Initialize networks\n",
    "Q_net = DQN(input_dim, output_dim).to(device)\n",
    "target_net = DQN(input_dim, output_dim).to(device)\n",
    "\n",
    "# Copy the weights from Q network to target network\n",
    "target_net.load_state_dict(Q_net.state_dict())\n",
    "\n",
    "# Put target network in \"no Training\" mode.\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=50000)    # The value is your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Starting DQN Training for Inventory Management\n",
      "======================================================================\n",
      "Goal: Minimize losses (negative rewards)\n",
      "Expected: Rewards start very negative, stabilize at less negative values\n",
      "======================================================================\n",
      "\n",
      "Episode 0/30000 | Steps: 50 | Avg Reward (100ep): -2293.89 | Epsilon: 1.000 | Avg MSE Loss: 0.0000 | Buffer: 50\n",
      "Episode 100/30000 | Steps: 50 | Avg Reward (100ep): -2274.04 | Epsilon: 0.996 | Avg MSE Loss: 0.3213 | Buffer: 5050\n",
      "Episode 200/30000 | Steps: 50 | Avg Reward (100ep): -2261.88 | Epsilon: 0.991 | Avg MSE Loss: 0.6550 | Buffer: 10050\n",
      "Episode 300/30000 | Steps: 50 | Avg Reward (100ep): -2271.32 | Epsilon: 0.987 | Avg MSE Loss: 0.7283 | Buffer: 15050\n",
      "Episode 400/30000 | Steps: 50 | Avg Reward (100ep): -2269.89 | Epsilon: 0.982 | Avg MSE Loss: 0.8331 | Buffer: 20050\n",
      "Episode 500/30000 | Steps: 50 | Avg Reward (100ep): -2254.31 | Epsilon: 0.978 | Avg MSE Loss: 0.9289 | Buffer: 25050\n",
      "Episode 600/30000 | Steps: 50 | Avg Reward (100ep): -2256.74 | Epsilon: 0.973 | Avg MSE Loss: 0.9705 | Buffer: 30050\n",
      "Episode 700/30000 | Steps: 50 | Avg Reward (100ep): -2253.44 | Epsilon: 0.969 | Avg MSE Loss: 0.9910 | Buffer: 35050\n",
      "Episode 800/30000 | Steps: 50 | Avg Reward (100ep): -2260.64 | Epsilon: 0.964 | Avg MSE Loss: 1.0596 | Buffer: 40050\n",
      "Episode 900/30000 | Steps: 50 | Avg Reward (100ep): -2255.05 | Epsilon: 0.960 | Avg MSE Loss: 1.0494 | Buffer: 45050\n",
      "Episode 1000/30000 | Steps: 50 | Avg Reward (100ep): -2263.38 | Epsilon: 0.955 | Avg MSE Loss: 1.0829 | Buffer: 50000\n",
      "Episode 1100/30000 | Steps: 50 | Avg Reward (100ep): -2244.05 | Epsilon: 0.951 | Avg MSE Loss: 1.0680 | Buffer: 50000\n",
      "Episode 1200/30000 | Steps: 50 | Avg Reward (100ep): -2250.60 | Epsilon: 0.946 | Avg MSE Loss: 1.0605 | Buffer: 50000\n",
      "Episode 1300/30000 | Steps: 50 | Avg Reward (100ep): -2258.66 | Epsilon: 0.942 | Avg MSE Loss: 1.0908 | Buffer: 50000\n",
      "Episode 1400/30000 | Steps: 50 | Avg Reward (100ep): -2262.81 | Epsilon: 0.937 | Avg MSE Loss: 1.0871 | Buffer: 50000\n",
      "Episode 1500/30000 | Steps: 50 | Avg Reward (100ep): -2236.72 | Epsilon: 0.932 | Avg MSE Loss: 1.0860 | Buffer: 50000\n",
      "Episode 1600/30000 | Steps: 50 | Avg Reward (100ep): -2269.13 | Epsilon: 0.928 | Avg MSE Loss: 1.0469 | Buffer: 50000\n",
      "Episode 1700/30000 | Steps: 50 | Avg Reward (100ep): -2259.12 | Epsilon: 0.923 | Avg MSE Loss: 1.0082 | Buffer: 50000\n",
      "Episode 1800/30000 | Steps: 50 | Avg Reward (100ep): -2264.50 | Epsilon: 0.919 | Avg MSE Loss: 1.0041 | Buffer: 50000\n",
      "Episode 1900/30000 | Steps: 50 | Avg Reward (100ep): -2240.72 | Epsilon: 0.914 | Avg MSE Loss: 0.9623 | Buffer: 50000\n",
      "Episode 2000/30000 | Steps: 50 | Avg Reward (100ep): -2249.97 | Epsilon: 0.910 | Avg MSE Loss: 0.9212 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 2000\n",
      "======================================================================\n",
      "  Initial Q-values: [4.15, 4.63, 4.01]\n",
      "  Total Reward: -2186.34 (-43.73 per step)\n",
      "  Actions: Nothing=0, Buy=34, Convert=16\n",
      "======================================================================\n",
      "\n",
      "Episode 2100/30000 | Steps: 50 | Avg Reward (100ep): -2241.87 | Epsilon: 0.905 | Avg MSE Loss: 0.8807 | Buffer: 50000\n",
      "Episode 2200/30000 | Steps: 50 | Avg Reward (100ep): -2250.16 | Epsilon: 0.901 | Avg MSE Loss: 0.8443 | Buffer: 50000\n",
      "Episode 2300/30000 | Steps: 50 | Avg Reward (100ep): -2249.39 | Epsilon: 0.896 | Avg MSE Loss: 0.8277 | Buffer: 50000\n",
      "Episode 2400/30000 | Steps: 50 | Avg Reward (100ep): -2249.12 | Epsilon: 0.892 | Avg MSE Loss: 0.8105 | Buffer: 50000\n",
      "Episode 2500/30000 | Steps: 50 | Avg Reward (100ep): -2240.54 | Epsilon: 0.887 | Avg MSE Loss: 0.7640 | Buffer: 50000\n",
      "Episode 2600/30000 | Steps: 50 | Avg Reward (100ep): -2243.36 | Epsilon: 0.883 | Avg MSE Loss: 0.7617 | Buffer: 50000\n",
      "Episode 2700/30000 | Steps: 50 | Avg Reward (100ep): -2214.58 | Epsilon: 0.879 | Avg MSE Loss: 0.7527 | Buffer: 50000\n",
      "Episode 2800/30000 | Steps: 50 | Avg Reward (100ep): -2249.36 | Epsilon: 0.874 | Avg MSE Loss: 0.7394 | Buffer: 50000\n",
      "Episode 2900/30000 | Steps: 50 | Avg Reward (100ep): -2246.90 | Epsilon: 0.869 | Avg MSE Loss: 0.7273 | Buffer: 50000\n",
      "Episode 3000/30000 | Steps: 50 | Avg Reward (100ep): -2205.02 | Epsilon: 0.865 | Avg MSE Loss: 0.7039 | Buffer: 50000\n",
      "Episode 3100/30000 | Steps: 50 | Avg Reward (100ep): -2221.77 | Epsilon: 0.861 | Avg MSE Loss: 0.6819 | Buffer: 50000\n",
      "Episode 3200/30000 | Steps: 50 | Avg Reward (100ep): -2245.06 | Epsilon: 0.856 | Avg MSE Loss: 0.6872 | Buffer: 50000\n",
      "Episode 3300/30000 | Steps: 50 | Avg Reward (100ep): -2212.80 | Epsilon: 0.852 | Avg MSE Loss: 0.6957 | Buffer: 50000\n",
      "Episode 3400/30000 | Steps: 50 | Avg Reward (100ep): -2221.22 | Epsilon: 0.847 | Avg MSE Loss: 0.6999 | Buffer: 50000\n",
      "Episode 3500/30000 | Steps: 50 | Avg Reward (100ep): -2229.13 | Epsilon: 0.843 | Avg MSE Loss: 0.6666 | Buffer: 50000\n",
      "Episode 3600/30000 | Steps: 50 | Avg Reward (100ep): -2201.95 | Epsilon: 0.838 | Avg MSE Loss: 0.6547 | Buffer: 50000\n",
      "Episode 3700/30000 | Steps: 50 | Avg Reward (100ep): -2213.23 | Epsilon: 0.834 | Avg MSE Loss: 0.6652 | Buffer: 50000\n",
      "Episode 3800/30000 | Steps: 50 | Avg Reward (100ep): -2196.73 | Epsilon: 0.829 | Avg MSE Loss: 0.6439 | Buffer: 50000\n",
      "Episode 3900/30000 | Steps: 50 | Avg Reward (100ep): -2214.60 | Epsilon: 0.825 | Avg MSE Loss: 0.6353 | Buffer: 50000\n",
      "Episode 4000/30000 | Steps: 50 | Avg Reward (100ep): -2216.72 | Epsilon: 0.820 | Avg MSE Loss: 0.6095 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 4000\n",
      "======================================================================\n",
      "  Initial Q-values: [3.35, 3.82, 3.25]\n",
      "  Total Reward: -2129.57 (-42.59 per step)\n",
      "  Actions: Nothing=0, Buy=34, Convert=16\n",
      "======================================================================\n",
      "\n",
      "Episode 4100/30000 | Steps: 50 | Avg Reward (100ep): -2196.20 | Epsilon: 0.816 | Avg MSE Loss: 0.6168 | Buffer: 50000\n",
      "Episode 4200/30000 | Steps: 50 | Avg Reward (100ep): -2206.03 | Epsilon: 0.811 | Avg MSE Loss: 0.6354 | Buffer: 50000\n",
      "Episode 4300/30000 | Steps: 50 | Avg Reward (100ep): -2193.84 | Epsilon: 0.806 | Avg MSE Loss: 0.6308 | Buffer: 50000\n",
      "Episode 4400/30000 | Steps: 50 | Avg Reward (100ep): -2192.55 | Epsilon: 0.802 | Avg MSE Loss: 0.6254 | Buffer: 50000\n",
      "Episode 4500/30000 | Steps: 50 | Avg Reward (100ep): -2206.89 | Epsilon: 0.797 | Avg MSE Loss: 0.6037 | Buffer: 50000\n",
      "Episode 4600/30000 | Steps: 50 | Avg Reward (100ep): -2222.57 | Epsilon: 0.793 | Avg MSE Loss: 0.6181 | Buffer: 50000\n",
      "Episode 4700/30000 | Steps: 50 | Avg Reward (100ep): -2167.67 | Epsilon: 0.788 | Avg MSE Loss: 0.6388 | Buffer: 50000\n",
      "Episode 4800/30000 | Steps: 50 | Avg Reward (100ep): -2196.82 | Epsilon: 0.784 | Avg MSE Loss: 0.6183 | Buffer: 50000\n",
      "Episode 4900/30000 | Steps: 50 | Avg Reward (100ep): -2187.17 | Epsilon: 0.779 | Avg MSE Loss: 0.6097 | Buffer: 50000\n",
      "Episode 5000/30000 | Steps: 50 | Avg Reward (100ep): -2211.40 | Epsilon: 0.775 | Avg MSE Loss: 0.6159 | Buffer: 50000\n",
      "Episode 5100/30000 | Steps: 50 | Avg Reward (100ep): -2207.98 | Epsilon: 0.770 | Avg MSE Loss: 0.6039 | Buffer: 50000\n",
      "Episode 5200/30000 | Steps: 50 | Avg Reward (100ep): -2191.53 | Epsilon: 0.766 | Avg MSE Loss: 0.6013 | Buffer: 50000\n",
      "Episode 5300/30000 | Steps: 50 | Avg Reward (100ep): -2185.60 | Epsilon: 0.762 | Avg MSE Loss: 0.5862 | Buffer: 50000\n",
      "Episode 5400/30000 | Steps: 50 | Avg Reward (100ep): -2175.45 | Epsilon: 0.757 | Avg MSE Loss: 0.6079 | Buffer: 50000\n",
      "Episode 5500/30000 | Steps: 50 | Avg Reward (100ep): -2197.21 | Epsilon: 0.752 | Avg MSE Loss: 0.5896 | Buffer: 50000\n",
      "Episode 5600/30000 | Steps: 50 | Avg Reward (100ep): -2180.96 | Epsilon: 0.748 | Avg MSE Loss: 0.5931 | Buffer: 50000\n",
      "Episode 5700/30000 | Steps: 50 | Avg Reward (100ep): -2193.33 | Epsilon: 0.744 | Avg MSE Loss: 0.5848 | Buffer: 50000\n",
      "Episode 5800/30000 | Steps: 50 | Avg Reward (100ep): -2175.49 | Epsilon: 0.739 | Avg MSE Loss: 0.6010 | Buffer: 50000\n",
      "Episode 5900/30000 | Steps: 50 | Avg Reward (100ep): -2173.12 | Epsilon: 0.734 | Avg MSE Loss: 0.6094 | Buffer: 50000\n",
      "Episode 6000/30000 | Steps: 50 | Avg Reward (100ep): -2182.83 | Epsilon: 0.730 | Avg MSE Loss: 0.5925 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 6000\n",
      "======================================================================\n",
      "  Initial Q-values: [3.28, 3.79, 3.07]\n",
      "  Total Reward: -1769.43 (-35.39 per step)\n",
      "  Actions: Nothing=0, Buy=34, Convert=16\n",
      "======================================================================\n",
      "\n",
      "Episode 6100/30000 | Steps: 50 | Avg Reward (100ep): -2159.13 | Epsilon: 0.726 | Avg MSE Loss: 0.5883 | Buffer: 50000\n",
      "Episode 6200/30000 | Steps: 50 | Avg Reward (100ep): -2178.11 | Epsilon: 0.721 | Avg MSE Loss: 0.5947 | Buffer: 50000\n",
      "Episode 6300/30000 | Steps: 50 | Avg Reward (100ep): -2160.46 | Epsilon: 0.717 | Avg MSE Loss: 0.5873 | Buffer: 50000\n",
      "Episode 6400/30000 | Steps: 50 | Avg Reward (100ep): -2179.16 | Epsilon: 0.712 | Avg MSE Loss: 0.5894 | Buffer: 50000\n",
      "Episode 6500/30000 | Steps: 50 | Avg Reward (100ep): -2176.00 | Epsilon: 0.708 | Avg MSE Loss: 0.5900 | Buffer: 50000\n",
      "Episode 6600/30000 | Steps: 50 | Avg Reward (100ep): -2155.48 | Epsilon: 0.703 | Avg MSE Loss: 0.5743 | Buffer: 50000\n",
      "Episode 6700/30000 | Steps: 50 | Avg Reward (100ep): -2165.41 | Epsilon: 0.699 | Avg MSE Loss: 0.5769 | Buffer: 50000\n",
      "Episode 6800/30000 | Steps: 50 | Avg Reward (100ep): -2175.21 | Epsilon: 0.694 | Avg MSE Loss: 0.5799 | Buffer: 50000\n",
      "Episode 6900/30000 | Steps: 50 | Avg Reward (100ep): -2171.08 | Epsilon: 0.690 | Avg MSE Loss: 0.5761 | Buffer: 50000\n",
      "Episode 7000/30000 | Steps: 50 | Avg Reward (100ep): -2195.10 | Epsilon: 0.685 | Avg MSE Loss: 0.5753 | Buffer: 50000\n",
      "Episode 7100/30000 | Steps: 50 | Avg Reward (100ep): -2178.51 | Epsilon: 0.680 | Avg MSE Loss: 0.5815 | Buffer: 50000\n",
      "Episode 7200/30000 | Steps: 50 | Avg Reward (100ep): -2178.96 | Epsilon: 0.676 | Avg MSE Loss: 0.5827 | Buffer: 50000\n",
      "Episode 7300/30000 | Steps: 50 | Avg Reward (100ep): -2180.00 | Epsilon: 0.671 | Avg MSE Loss: 0.5929 | Buffer: 50000\n",
      "Episode 7400/30000 | Steps: 50 | Avg Reward (100ep): -2173.27 | Epsilon: 0.667 | Avg MSE Loss: 0.5828 | Buffer: 50000\n",
      "Episode 7500/30000 | Steps: 50 | Avg Reward (100ep): -2167.57 | Epsilon: 0.662 | Avg MSE Loss: 0.5898 | Buffer: 50000\n",
      "Episode 7600/30000 | Steps: 50 | Avg Reward (100ep): -2162.11 | Epsilon: 0.658 | Avg MSE Loss: 0.5793 | Buffer: 50000\n",
      "Episode 7700/30000 | Steps: 50 | Avg Reward (100ep): -2160.38 | Epsilon: 0.653 | Avg MSE Loss: 0.5912 | Buffer: 50000\n",
      "Episode 7800/30000 | Steps: 50 | Avg Reward (100ep): -2159.91 | Epsilon: 0.649 | Avg MSE Loss: 0.5791 | Buffer: 50000\n",
      "Episode 7900/30000 | Steps: 50 | Avg Reward (100ep): -2144.23 | Epsilon: 0.645 | Avg MSE Loss: 0.5874 | Buffer: 50000\n",
      "Episode 8000/30000 | Steps: 150 | Avg Reward (100ep): -2186.65 | Epsilon: 0.640 | Avg MSE Loss: 0.5859 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 8000\n",
      "======================================================================\n",
      "  Initial Q-values: [3.01, 3.45, 2.93]\n",
      "  Total Reward: -6557.21 (-43.71 per step)\n",
      "  Actions: Nothing=0, Buy=58, Convert=92\n",
      "======================================================================\n",
      "\n",
      "Episode 8100/30000 | Steps: 150 | Avg Reward (100ep): -6426.77 | Epsilon: 0.635 | Avg MSE Loss: 0.5796 | Buffer: 50000\n",
      "Episode 8200/30000 | Steps: 150 | Avg Reward (100ep): -6420.35 | Epsilon: 0.631 | Avg MSE Loss: 0.5791 | Buffer: 50000\n",
      "Episode 8300/30000 | Steps: 150 | Avg Reward (100ep): -6408.53 | Epsilon: 0.627 | Avg MSE Loss: 0.5846 | Buffer: 50000\n",
      "Episode 8400/30000 | Steps: 150 | Avg Reward (100ep): -6413.75 | Epsilon: 0.622 | Avg MSE Loss: 0.5763 | Buffer: 50000\n",
      "Episode 8500/30000 | Steps: 150 | Avg Reward (100ep): -6400.51 | Epsilon: 0.617 | Avg MSE Loss: 0.5742 | Buffer: 50000\n",
      "Episode 8600/30000 | Steps: 150 | Avg Reward (100ep): -6365.87 | Epsilon: 0.613 | Avg MSE Loss: 0.5733 | Buffer: 50000\n",
      "Episode 8700/30000 | Steps: 150 | Avg Reward (100ep): -6424.04 | Epsilon: 0.609 | Avg MSE Loss: 0.5577 | Buffer: 50000\n",
      "Episode 8800/30000 | Steps: 150 | Avg Reward (100ep): -6430.50 | Epsilon: 0.604 | Avg MSE Loss: 0.5552 | Buffer: 50000\n",
      "Episode 8900/30000 | Steps: 150 | Avg Reward (100ep): -6382.63 | Epsilon: 0.599 | Avg MSE Loss: 0.5526 | Buffer: 50000\n",
      "Episode 9000/30000 | Steps: 150 | Avg Reward (100ep): -6410.56 | Epsilon: 0.595 | Avg MSE Loss: 0.5483 | Buffer: 50000\n",
      "Episode 9100/30000 | Steps: 150 | Avg Reward (100ep): -6384.21 | Epsilon: 0.591 | Avg MSE Loss: 0.5537 | Buffer: 50000\n",
      "Episode 9200/30000 | Steps: 150 | Avg Reward (100ep): -6393.32 | Epsilon: 0.586 | Avg MSE Loss: 0.5515 | Buffer: 50000\n",
      "Episode 9300/30000 | Steps: 150 | Avg Reward (100ep): -6337.61 | Epsilon: 0.582 | Avg MSE Loss: 0.5491 | Buffer: 50000\n",
      "Episode 9400/30000 | Steps: 150 | Avg Reward (100ep): -6353.87 | Epsilon: 0.577 | Avg MSE Loss: 0.5543 | Buffer: 50000\n",
      "Episode 9500/30000 | Steps: 150 | Avg Reward (100ep): -6366.85 | Epsilon: 0.573 | Avg MSE Loss: 0.5477 | Buffer: 50000\n",
      "Episode 9600/30000 | Steps: 150 | Avg Reward (100ep): -6336.71 | Epsilon: 0.568 | Avg MSE Loss: 0.5425 | Buffer: 50000\n",
      "Episode 9700/30000 | Steps: 150 | Avg Reward (100ep): -6320.85 | Epsilon: 0.564 | Avg MSE Loss: 0.5338 | Buffer: 50000\n",
      "Episode 9800/30000 | Steps: 150 | Avg Reward (100ep): -6370.39 | Epsilon: 0.559 | Avg MSE Loss: 0.5284 | Buffer: 50000\n",
      "Episode 9900/30000 | Steps: 150 | Avg Reward (100ep): -6346.56 | Epsilon: 0.554 | Avg MSE Loss: 0.5268 | Buffer: 50000\n",
      "Episode 10000/30000 | Steps: 150 | Avg Reward (100ep): -6339.92 | Epsilon: 0.550 | Avg MSE Loss: 0.5327 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 10000\n",
      "======================================================================\n",
      "  Initial Q-values: [2.79, 3.23, 2.59]\n",
      "  Total Reward: -6019.41 (-40.13 per step)\n",
      "  Actions: Nothing=0, Buy=100, Convert=50\n",
      "======================================================================\n",
      "\n",
      "Episode 10100/30000 | Steps: 150 | Avg Reward (100ep): -6344.52 | Epsilon: 0.545 | Avg MSE Loss: 0.5364 | Buffer: 50000\n",
      "Episode 10200/30000 | Steps: 150 | Avg Reward (100ep): -6301.45 | Epsilon: 0.541 | Avg MSE Loss: 0.5378 | Buffer: 50000\n",
      "Episode 10300/30000 | Steps: 150 | Avg Reward (100ep): -6347.42 | Epsilon: 0.536 | Avg MSE Loss: 0.5342 | Buffer: 50000\n",
      "Episode 10400/30000 | Steps: 150 | Avg Reward (100ep): -6277.76 | Epsilon: 0.532 | Avg MSE Loss: 0.5312 | Buffer: 50000\n",
      "Episode 10500/30000 | Steps: 150 | Avg Reward (100ep): -6383.48 | Epsilon: 0.528 | Avg MSE Loss: 0.5343 | Buffer: 50000\n",
      "Episode 10600/30000 | Steps: 150 | Avg Reward (100ep): -6301.62 | Epsilon: 0.523 | Avg MSE Loss: 0.5292 | Buffer: 50000\n",
      "Episode 10700/30000 | Steps: 150 | Avg Reward (100ep): -6318.60 | Epsilon: 0.518 | Avg MSE Loss: 0.5343 | Buffer: 50000\n",
      "Episode 10800/30000 | Steps: 150 | Avg Reward (100ep): -6310.41 | Epsilon: 0.514 | Avg MSE Loss: 0.5424 | Buffer: 50000\n",
      "Episode 10900/30000 | Steps: 150 | Avg Reward (100ep): -6297.17 | Epsilon: 0.510 | Avg MSE Loss: 0.5489 | Buffer: 50000\n",
      "Episode 11000/30000 | Steps: 150 | Avg Reward (100ep): -6306.37 | Epsilon: 0.505 | Avg MSE Loss: 0.5422 | Buffer: 50000\n",
      "Episode 11100/30000 | Steps: 150 | Avg Reward (100ep): -6316.64 | Epsilon: 0.500 | Avg MSE Loss: 0.5368 | Buffer: 50000\n",
      "Episode 11200/30000 | Steps: 150 | Avg Reward (100ep): -6266.41 | Epsilon: 0.496 | Avg MSE Loss: 0.5259 | Buffer: 50000\n",
      "Episode 11300/30000 | Steps: 150 | Avg Reward (100ep): -6276.45 | Epsilon: 0.492 | Avg MSE Loss: 0.5222 | Buffer: 50000\n",
      "Episode 11400/30000 | Steps: 150 | Avg Reward (100ep): -6267.18 | Epsilon: 0.487 | Avg MSE Loss: 0.5193 | Buffer: 50000\n",
      "Episode 11500/30000 | Steps: 150 | Avg Reward (100ep): -6294.00 | Epsilon: 0.483 | Avg MSE Loss: 0.5177 | Buffer: 50000\n",
      "Episode 11600/30000 | Steps: 150 | Avg Reward (100ep): -6284.11 | Epsilon: 0.478 | Avg MSE Loss: 0.5210 | Buffer: 50000\n",
      "Episode 11700/30000 | Steps: 150 | Avg Reward (100ep): -6264.99 | Epsilon: 0.474 | Avg MSE Loss: 0.5192 | Buffer: 50000\n",
      "Episode 11800/30000 | Steps: 150 | Avg Reward (100ep): -6244.55 | Epsilon: 0.469 | Avg MSE Loss: 0.5229 | Buffer: 50000\n",
      "Episode 11900/30000 | Steps: 150 | Avg Reward (100ep): -6288.54 | Epsilon: 0.465 | Avg MSE Loss: 0.5270 | Buffer: 50000\n",
      "Episode 12000/30000 | Steps: 150 | Avg Reward (100ep): -6225.26 | Epsilon: 0.460 | Avg MSE Loss: 0.5267 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 12000\n",
      "======================================================================\n",
      "  Initial Q-values: [2.06, 2.57, 1.96]\n",
      "  Total Reward: -5639.54 (-37.60 per step)\n",
      "  Actions: Nothing=0, Buy=100, Convert=50\n",
      "======================================================================\n",
      "\n",
      "Episode 12100/30000 | Steps: 150 | Avg Reward (100ep): -6231.18 | Epsilon: 0.456 | Avg MSE Loss: 0.5186 | Buffer: 50000\n",
      "Episode 12200/30000 | Steps: 150 | Avg Reward (100ep): -6273.47 | Epsilon: 0.451 | Avg MSE Loss: 0.5151 | Buffer: 50000\n",
      "Episode 12300/30000 | Steps: 150 | Avg Reward (100ep): -6250.28 | Epsilon: 0.447 | Avg MSE Loss: 0.5156 | Buffer: 50000\n",
      "Episode 12400/30000 | Steps: 150 | Avg Reward (100ep): -6194.74 | Epsilon: 0.442 | Avg MSE Loss: 0.5122 | Buffer: 50000\n",
      "Episode 12500/30000 | Steps: 150 | Avg Reward (100ep): -6224.38 | Epsilon: 0.438 | Avg MSE Loss: 0.5162 | Buffer: 50000\n",
      "Episode 12600/30000 | Steps: 150 | Avg Reward (100ep): -6182.02 | Epsilon: 0.433 | Avg MSE Loss: 0.5144 | Buffer: 50000\n",
      "Episode 12700/30000 | Steps: 150 | Avg Reward (100ep): -6219.54 | Epsilon: 0.428 | Avg MSE Loss: 0.5098 | Buffer: 50000\n",
      "Episode 12800/30000 | Steps: 150 | Avg Reward (100ep): -6198.36 | Epsilon: 0.424 | Avg MSE Loss: 0.5115 | Buffer: 50000\n",
      "Episode 12900/30000 | Steps: 150 | Avg Reward (100ep): -6187.45 | Epsilon: 0.419 | Avg MSE Loss: 0.5127 | Buffer: 50000\n",
      "Episode 13000/30000 | Steps: 150 | Avg Reward (100ep): -6207.65 | Epsilon: 0.415 | Avg MSE Loss: 0.5079 | Buffer: 50000\n",
      "Episode 13100/30000 | Steps: 150 | Avg Reward (100ep): -6219.93 | Epsilon: 0.410 | Avg MSE Loss: 0.5081 | Buffer: 50000\n",
      "Episode 13200/30000 | Steps: 150 | Avg Reward (100ep): -6221.81 | Epsilon: 0.406 | Avg MSE Loss: 0.5027 | Buffer: 50000\n",
      "Episode 13300/30000 | Steps: 150 | Avg Reward (100ep): -6172.43 | Epsilon: 0.401 | Avg MSE Loss: 0.5083 | Buffer: 50000\n",
      "Episode 13400/30000 | Steps: 150 | Avg Reward (100ep): -6203.55 | Epsilon: 0.397 | Avg MSE Loss: 0.5069 | Buffer: 50000\n",
      "Episode 13500/30000 | Steps: 150 | Avg Reward (100ep): -6186.01 | Epsilon: 0.392 | Avg MSE Loss: 0.5058 | Buffer: 50000\n",
      "Episode 13600/30000 | Steps: 150 | Avg Reward (100ep): -6169.67 | Epsilon: 0.388 | Avg MSE Loss: 0.5138 | Buffer: 50000\n",
      "Episode 13700/30000 | Steps: 150 | Avg Reward (100ep): -6170.27 | Epsilon: 0.383 | Avg MSE Loss: 0.5149 | Buffer: 50000\n",
      "Episode 13800/30000 | Steps: 150 | Avg Reward (100ep): -6188.11 | Epsilon: 0.379 | Avg MSE Loss: 0.5147 | Buffer: 50000\n",
      "Episode 13900/30000 | Steps: 150 | Avg Reward (100ep): -6173.37 | Epsilon: 0.375 | Avg MSE Loss: 0.5073 | Buffer: 50000\n",
      "Episode 14000/30000 | Steps: 150 | Avg Reward (100ep): -6157.31 | Epsilon: 0.370 | Avg MSE Loss: 0.5072 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 14000\n",
      "======================================================================\n",
      "  Initial Q-values: [1.48, 1.99, 1.39]\n",
      "  Total Reward: -5750.31 (-38.34 per step)\n",
      "  Actions: Nothing=0, Buy=100, Convert=50\n",
      "======================================================================\n",
      "\n",
      "Episode 14100/30000 | Steps: 150 | Avg Reward (100ep): -6133.20 | Epsilon: 0.366 | Avg MSE Loss: 0.5074 | Buffer: 50000\n",
      "Episode 14200/30000 | Steps: 150 | Avg Reward (100ep): -6169.74 | Epsilon: 0.361 | Avg MSE Loss: 0.5041 | Buffer: 50000\n",
      "Episode 14300/30000 | Steps: 150 | Avg Reward (100ep): -6143.21 | Epsilon: 0.357 | Avg MSE Loss: 0.4972 | Buffer: 50000\n",
      "Episode 14400/30000 | Steps: 150 | Avg Reward (100ep): -6108.13 | Epsilon: 0.352 | Avg MSE Loss: 0.4930 | Buffer: 50000\n",
      "Episode 14500/30000 | Steps: 150 | Avg Reward (100ep): -6169.17 | Epsilon: 0.348 | Avg MSE Loss: 0.4948 | Buffer: 50000\n",
      "Episode 14600/30000 | Steps: 150 | Avg Reward (100ep): -6137.23 | Epsilon: 0.343 | Avg MSE Loss: 0.4968 | Buffer: 50000\n",
      "Episode 14700/30000 | Steps: 150 | Avg Reward (100ep): -6141.68 | Epsilon: 0.339 | Avg MSE Loss: 0.4970 | Buffer: 50000\n",
      "Episode 14800/30000 | Steps: 150 | Avg Reward (100ep): -6148.56 | Epsilon: 0.334 | Avg MSE Loss: 0.4976 | Buffer: 50000\n",
      "Episode 14900/30000 | Steps: 150 | Avg Reward (100ep): -6133.87 | Epsilon: 0.330 | Avg MSE Loss: 0.5030 | Buffer: 50000\n",
      "Episode 15000/30000 | Steps: 150 | Avg Reward (100ep): -6145.90 | Epsilon: 0.325 | Avg MSE Loss: 0.5054 | Buffer: 50000\n",
      "Episode 15100/30000 | Steps: 150 | Avg Reward (100ep): -6072.62 | Epsilon: 0.321 | Avg MSE Loss: 0.5059 | Buffer: 50000\n",
      "Episode 15200/30000 | Steps: 150 | Avg Reward (100ep): -6113.55 | Epsilon: 0.316 | Avg MSE Loss: 0.5024 | Buffer: 50000\n",
      "Episode 15300/30000 | Steps: 150 | Avg Reward (100ep): -6107.77 | Epsilon: 0.311 | Avg MSE Loss: 0.4925 | Buffer: 50000\n",
      "Episode 15400/30000 | Steps: 150 | Avg Reward (100ep): -6100.69 | Epsilon: 0.307 | Avg MSE Loss: 0.4916 | Buffer: 50000\n",
      "Episode 15500/30000 | Steps: 150 | Avg Reward (100ep): -6059.42 | Epsilon: 0.302 | Avg MSE Loss: 0.4908 | Buffer: 50000\n",
      "Episode 15600/30000 | Steps: 150 | Avg Reward (100ep): -6107.19 | Epsilon: 0.298 | Avg MSE Loss: 0.4949 | Buffer: 50000\n",
      "Episode 15700/30000 | Steps: 150 | Avg Reward (100ep): -6106.12 | Epsilon: 0.293 | Avg MSE Loss: 0.4881 | Buffer: 50000\n",
      "Episode 15800/30000 | Steps: 150 | Avg Reward (100ep): -6094.13 | Epsilon: 0.289 | Avg MSE Loss: 0.4912 | Buffer: 50000\n",
      "Episode 15900/30000 | Steps: 150 | Avg Reward (100ep): -6068.22 | Epsilon: 0.284 | Avg MSE Loss: 0.4896 | Buffer: 50000\n",
      "Episode 16000/30000 | Steps: 150 | Avg Reward (100ep): -6040.15 | Epsilon: 0.280 | Avg MSE Loss: 0.4889 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 16000\n",
      "======================================================================\n",
      "  Initial Q-values: [0.84, 1.43, 0.79]\n",
      "  Total Reward: -5679.59 (-37.86 per step)\n",
      "  Actions: Nothing=0, Buy=100, Convert=50\n",
      "======================================================================\n",
      "\n",
      "Episode 16100/30000 | Steps: 150 | Avg Reward (100ep): -6098.09 | Epsilon: 0.275 | Avg MSE Loss: 0.4925 | Buffer: 50000\n",
      "Episode 16200/30000 | Steps: 150 | Avg Reward (100ep): -6048.04 | Epsilon: 0.271 | Avg MSE Loss: 0.4964 | Buffer: 50000\n",
      "Episode 16300/30000 | Steps: 150 | Avg Reward (100ep): -6086.17 | Epsilon: 0.266 | Avg MSE Loss: 0.4993 | Buffer: 50000\n",
      "Episode 16400/30000 | Steps: 150 | Avg Reward (100ep): -6050.58 | Epsilon: 0.262 | Avg MSE Loss: 0.4938 | Buffer: 50000\n",
      "Episode 16500/30000 | Steps: 150 | Avg Reward (100ep): -6084.83 | Epsilon: 0.257 | Avg MSE Loss: 0.4882 | Buffer: 50000\n",
      "Episode 16600/30000 | Steps: 150 | Avg Reward (100ep): -6041.55 | Epsilon: 0.253 | Avg MSE Loss: 0.4840 | Buffer: 50000\n",
      "Episode 16700/30000 | Steps: 150 | Avg Reward (100ep): -6047.67 | Epsilon: 0.249 | Avg MSE Loss: 0.4820 | Buffer: 50000\n",
      "Episode 16800/30000 | Steps: 150 | Avg Reward (100ep): -6038.91 | Epsilon: 0.244 | Avg MSE Loss: 0.4852 | Buffer: 50000\n",
      "Episode 16900/30000 | Steps: 150 | Avg Reward (100ep): -6027.20 | Epsilon: 0.240 | Avg MSE Loss: 0.4860 | Buffer: 50000\n",
      "Episode 17000/30000 | Steps: 150 | Avg Reward (100ep): -6012.26 | Epsilon: 0.235 | Avg MSE Loss: 0.4864 | Buffer: 50000\n",
      "Episode 17100/30000 | Steps: 150 | Avg Reward (100ep): -5989.03 | Epsilon: 0.231 | Avg MSE Loss: 0.4915 | Buffer: 50000\n",
      "Episode 17200/30000 | Steps: 150 | Avg Reward (100ep): -6003.22 | Epsilon: 0.226 | Avg MSE Loss: 0.4962 | Buffer: 50000\n",
      "Episode 17300/30000 | Steps: 150 | Avg Reward (100ep): -5988.09 | Epsilon: 0.222 | Avg MSE Loss: 0.4983 | Buffer: 50000\n",
      "Episode 17400/30000 | Steps: 150 | Avg Reward (100ep): -6015.55 | Epsilon: 0.217 | Avg MSE Loss: 0.4888 | Buffer: 50000\n",
      "Episode 17500/30000 | Steps: 150 | Avg Reward (100ep): -6019.04 | Epsilon: 0.213 | Avg MSE Loss: 0.4852 | Buffer: 50000\n",
      "Episode 17600/30000 | Steps: 150 | Avg Reward (100ep): -5987.36 | Epsilon: 0.208 | Avg MSE Loss: 0.4735 | Buffer: 50000\n",
      "Episode 17700/30000 | Steps: 150 | Avg Reward (100ep): -5973.31 | Epsilon: 0.204 | Avg MSE Loss: 0.4758 | Buffer: 50000\n",
      "Episode 17800/30000 | Steps: 150 | Avg Reward (100ep): -5994.24 | Epsilon: 0.199 | Avg MSE Loss: 0.4770 | Buffer: 50000\n",
      "Episode 17900/30000 | Steps: 150 | Avg Reward (100ep): -5963.50 | Epsilon: 0.195 | Avg MSE Loss: 0.4731 | Buffer: 50000\n",
      "Episode 18000/30000 | Steps: 300 | Avg Reward (100ep): -6015.33 | Epsilon: 0.190 | Avg MSE Loss: 0.4749 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 18000\n",
      "======================================================================\n",
      "  Initial Q-values: [0.25, 0.78, 0.09]\n",
      "  Total Reward: -10977.82 (-36.59 per step)\n",
      "  Actions: Nothing=0, Buy=200, Convert=100\n",
      "======================================================================\n",
      "\n",
      "Episode 18100/30000 | Steps: 300 | Avg Reward (100ep): -11926.29 | Epsilon: 0.185 | Avg MSE Loss: 0.4703 | Buffer: 50000\n",
      "Episode 18200/30000 | Steps: 300 | Avg Reward (100ep): -11887.92 | Epsilon: 0.181 | Avg MSE Loss: 0.4688 | Buffer: 50000\n",
      "Episode 18300/30000 | Steps: 300 | Avg Reward (100ep): -11897.82 | Epsilon: 0.176 | Avg MSE Loss: 0.4674 | Buffer: 50000\n",
      "Episode 18400/30000 | Steps: 300 | Avg Reward (100ep): -11917.86 | Epsilon: 0.172 | Avg MSE Loss: 0.4703 | Buffer: 50000\n",
      "Episode 18500/30000 | Steps: 300 | Avg Reward (100ep): -11977.18 | Epsilon: 0.167 | Avg MSE Loss: 0.4757 | Buffer: 50000\n",
      "Episode 18600/30000 | Steps: 300 | Avg Reward (100ep): -11918.05 | Epsilon: 0.163 | Avg MSE Loss: 0.4741 | Buffer: 50000\n",
      "Episode 18700/30000 | Steps: 300 | Avg Reward (100ep): -11865.19 | Epsilon: 0.158 | Avg MSE Loss: 0.4749 | Buffer: 50000\n",
      "Episode 18800/30000 | Steps: 300 | Avg Reward (100ep): -11831.66 | Epsilon: 0.154 | Avg MSE Loss: 0.4742 | Buffer: 50000\n",
      "Episode 18900/30000 | Steps: 300 | Avg Reward (100ep): -11879.49 | Epsilon: 0.149 | Avg MSE Loss: 0.4747 | Buffer: 50000\n",
      "Episode 19000/30000 | Steps: 300 | Avg Reward (100ep): -11817.14 | Epsilon: 0.145 | Avg MSE Loss: 0.4743 | Buffer: 50000\n",
      "Episode 19100/30000 | Steps: 300 | Avg Reward (100ep): -11855.70 | Epsilon: 0.140 | Avg MSE Loss: 0.4746 | Buffer: 50000\n",
      "Episode 19200/30000 | Steps: 300 | Avg Reward (100ep): -11823.50 | Epsilon: 0.136 | Avg MSE Loss: 0.4708 | Buffer: 50000\n",
      "Episode 19300/30000 | Steps: 300 | Avg Reward (100ep): -11820.96 | Epsilon: 0.131 | Avg MSE Loss: 0.4661 | Buffer: 50000\n",
      "Episode 19400/30000 | Steps: 300 | Avg Reward (100ep): -11826.61 | Epsilon: 0.127 | Avg MSE Loss: 0.4701 | Buffer: 50000\n",
      "Episode 19500/30000 | Steps: 300 | Avg Reward (100ep): -11773.72 | Epsilon: 0.123 | Avg MSE Loss: 0.4660 | Buffer: 50000\n",
      "Episode 19600/30000 | Steps: 300 | Avg Reward (100ep): -11785.21 | Epsilon: 0.118 | Avg MSE Loss: 0.4692 | Buffer: 50000\n",
      "Episode 19700/30000 | Steps: 300 | Avg Reward (100ep): -11764.76 | Epsilon: 0.114 | Avg MSE Loss: 0.4679 | Buffer: 50000\n",
      "Episode 19800/30000 | Steps: 300 | Avg Reward (100ep): -11752.06 | Epsilon: 0.109 | Avg MSE Loss: 0.4631 | Buffer: 50000\n",
      "Episode 19900/30000 | Steps: 300 | Avg Reward (100ep): -11781.45 | Epsilon: 0.105 | Avg MSE Loss: 0.4626 | Buffer: 50000\n",
      "Episode 20000/30000 | Steps: 300 | Avg Reward (100ep): -11717.37 | Epsilon: 0.100 | Avg MSE Loss: 0.4629 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 20000\n",
      "======================================================================\n",
      "  Initial Q-values: [-0.12, 0.31, -0.35]\n",
      "  Total Reward: -11813.23 (-39.38 per step)\n",
      "  Actions: Nothing=0, Buy=200, Convert=100\n",
      "======================================================================\n",
      "\n",
      "Episode 20100/30000 | Steps: 300 | Avg Reward (100ep): -11759.06 | Epsilon: 0.100 | Avg MSE Loss: 0.4582 | Buffer: 50000\n",
      "Episode 20200/30000 | Steps: 300 | Avg Reward (100ep): -11830.00 | Epsilon: 0.100 | Avg MSE Loss: 0.4660 | Buffer: 50000\n",
      "Episode 20300/30000 | Steps: 300 | Avg Reward (100ep): -11714.71 | Epsilon: 0.100 | Avg MSE Loss: 0.4761 | Buffer: 50000\n",
      "Episode 20400/30000 | Steps: 300 | Avg Reward (100ep): -11723.08 | Epsilon: 0.100 | Avg MSE Loss: 0.4662 | Buffer: 50000\n",
      "Episode 20500/30000 | Steps: 300 | Avg Reward (100ep): -11757.74 | Epsilon: 0.100 | Avg MSE Loss: 0.4650 | Buffer: 50000\n",
      "Episode 20600/30000 | Steps: 300 | Avg Reward (100ep): -11779.20 | Epsilon: 0.100 | Avg MSE Loss: 0.4728 | Buffer: 50000\n",
      "Episode 20700/30000 | Steps: 300 | Avg Reward (100ep): -11778.25 | Epsilon: 0.100 | Avg MSE Loss: 0.4713 | Buffer: 50000\n",
      "Episode 20800/30000 | Steps: 300 | Avg Reward (100ep): -11862.41 | Epsilon: 0.100 | Avg MSE Loss: 0.4713 | Buffer: 50000\n",
      "Episode 20900/30000 | Steps: 300 | Avg Reward (100ep): -11706.21 | Epsilon: 0.100 | Avg MSE Loss: 0.4698 | Buffer: 50000\n",
      "Episode 21000/30000 | Steps: 300 | Avg Reward (100ep): -11687.77 | Epsilon: 0.100 | Avg MSE Loss: 0.4632 | Buffer: 50000\n",
      "Episode 21100/30000 | Steps: 300 | Avg Reward (100ep): -11745.07 | Epsilon: 0.100 | Avg MSE Loss: 0.4680 | Buffer: 50000\n",
      "Episode 21200/30000 | Steps: 300 | Avg Reward (100ep): -11754.56 | Epsilon: 0.100 | Avg MSE Loss: 0.4783 | Buffer: 50000\n",
      "Episode 21300/30000 | Steps: 300 | Avg Reward (100ep): -11766.71 | Epsilon: 0.100 | Avg MSE Loss: 0.4738 | Buffer: 50000\n",
      "Episode 21400/30000 | Steps: 300 | Avg Reward (100ep): -11774.07 | Epsilon: 0.100 | Avg MSE Loss: 0.4680 | Buffer: 50000\n",
      "Episode 21500/30000 | Steps: 300 | Avg Reward (100ep): -11772.58 | Epsilon: 0.100 | Avg MSE Loss: 0.4620 | Buffer: 50000\n",
      "Episode 21600/30000 | Steps: 300 | Avg Reward (100ep): -11832.10 | Epsilon: 0.100 | Avg MSE Loss: 0.4634 | Buffer: 50000\n",
      "Episode 21700/30000 | Steps: 300 | Avg Reward (100ep): -11775.42 | Epsilon: 0.100 | Avg MSE Loss: 0.4602 | Buffer: 50000\n",
      "Episode 21800/30000 | Steps: 300 | Avg Reward (100ep): -11762.88 | Epsilon: 0.100 | Avg MSE Loss: 0.4569 | Buffer: 50000\n",
      "Episode 21900/30000 | Steps: 300 | Avg Reward (100ep): -11805.56 | Epsilon: 0.100 | Avg MSE Loss: 0.4645 | Buffer: 50000\n",
      "Episode 22000/30000 | Steps: 300 | Avg Reward (100ep): -11768.34 | Epsilon: 0.100 | Avg MSE Loss: 0.4734 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 22000\n",
      "======================================================================\n",
      "  Initial Q-values: [0.20, 0.68, 0.11]\n",
      "  Total Reward: -11027.28 (-36.76 per step)\n",
      "  Actions: Nothing=0, Buy=200, Convert=100\n",
      "======================================================================\n",
      "\n",
      "Episode 22100/30000 | Steps: 300 | Avg Reward (100ep): -11789.33 | Epsilon: 0.100 | Avg MSE Loss: 0.4735 | Buffer: 50000\n",
      "Episode 22200/30000 | Steps: 300 | Avg Reward (100ep): -11772.13 | Epsilon: 0.100 | Avg MSE Loss: 0.4688 | Buffer: 50000\n",
      "Episode 22300/30000 | Steps: 300 | Avg Reward (100ep): -11797.61 | Epsilon: 0.100 | Avg MSE Loss: 0.4625 | Buffer: 50000\n",
      "Episode 22400/30000 | Steps: 300 | Avg Reward (100ep): -11746.34 | Epsilon: 0.100 | Avg MSE Loss: 0.4655 | Buffer: 50000\n",
      "Episode 22500/30000 | Steps: 300 | Avg Reward (100ep): -11770.51 | Epsilon: 0.100 | Avg MSE Loss: 0.4627 | Buffer: 50000\n",
      "Episode 22600/30000 | Steps: 300 | Avg Reward (100ep): -11730.05 | Epsilon: 0.100 | Avg MSE Loss: 0.4661 | Buffer: 50000\n",
      "Episode 22700/30000 | Steps: 300 | Avg Reward (100ep): -11799.29 | Epsilon: 0.100 | Avg MSE Loss: 0.4719 | Buffer: 50000\n",
      "Episode 22800/30000 | Steps: 300 | Avg Reward (100ep): -11747.78 | Epsilon: 0.100 | Avg MSE Loss: 0.4715 | Buffer: 50000\n",
      "Episode 22900/30000 | Steps: 300 | Avg Reward (100ep): -11680.41 | Epsilon: 0.100 | Avg MSE Loss: 0.4636 | Buffer: 50000\n",
      "Episode 23000/30000 | Steps: 300 | Avg Reward (100ep): -11728.89 | Epsilon: 0.100 | Avg MSE Loss: 0.4637 | Buffer: 50000\n",
      "Episode 23100/30000 | Steps: 300 | Avg Reward (100ep): -11752.43 | Epsilon: 0.100 | Avg MSE Loss: 0.4621 | Buffer: 50000\n",
      "Episode 23200/30000 | Steps: 300 | Avg Reward (100ep): -11747.50 | Epsilon: 0.100 | Avg MSE Loss: 0.4605 | Buffer: 50000\n",
      "Episode 23300/30000 | Steps: 300 | Avg Reward (100ep): -11750.79 | Epsilon: 0.100 | Avg MSE Loss: 0.4642 | Buffer: 50000\n",
      "Episode 23400/30000 | Steps: 300 | Avg Reward (100ep): -11783.16 | Epsilon: 0.100 | Avg MSE Loss: 0.4649 | Buffer: 50000\n",
      "Episode 23500/30000 | Steps: 300 | Avg Reward (100ep): -11781.04 | Epsilon: 0.100 | Avg MSE Loss: 0.4648 | Buffer: 50000\n",
      "Episode 23600/30000 | Steps: 300 | Avg Reward (100ep): -11749.28 | Epsilon: 0.100 | Avg MSE Loss: 0.4626 | Buffer: 50000\n",
      "Episode 23700/30000 | Steps: 300 | Avg Reward (100ep): -11726.64 | Epsilon: 0.100 | Avg MSE Loss: 0.4620 | Buffer: 50000\n",
      "Episode 23800/30000 | Steps: 300 | Avg Reward (100ep): -11745.60 | Epsilon: 0.100 | Avg MSE Loss: 0.4666 | Buffer: 50000\n",
      "Episode 23900/30000 | Steps: 300 | Avg Reward (100ep): -11740.52 | Epsilon: 0.100 | Avg MSE Loss: 0.4726 | Buffer: 50000\n",
      "Episode 24000/30000 | Steps: 300 | Avg Reward (100ep): -11746.89 | Epsilon: 0.100 | Avg MSE Loss: 0.4761 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 24000\n",
      "======================================================================\n",
      "  Initial Q-values: [-0.41, 0.14, -0.47]\n",
      "  Total Reward: -12121.62 (-40.41 per step)\n",
      "  Actions: Nothing=0, Buy=201, Convert=99\n",
      "======================================================================\n",
      "\n",
      "Episode 24100/30000 | Steps: 300 | Avg Reward (100ep): -11806.46 | Epsilon: 0.100 | Avg MSE Loss: 0.4675 | Buffer: 50000\n",
      "Episode 24200/30000 | Steps: 300 | Avg Reward (100ep): -11766.49 | Epsilon: 0.100 | Avg MSE Loss: 0.4655 | Buffer: 50000\n",
      "Episode 24300/30000 | Steps: 300 | Avg Reward (100ep): -11751.64 | Epsilon: 0.100 | Avg MSE Loss: 0.4697 | Buffer: 50000\n",
      "Episode 24400/30000 | Steps: 300 | Avg Reward (100ep): -11772.56 | Epsilon: 0.100 | Avg MSE Loss: 0.4689 | Buffer: 50000\n",
      "Episode 24500/30000 | Steps: 300 | Avg Reward (100ep): -11723.25 | Epsilon: 0.100 | Avg MSE Loss: 0.4705 | Buffer: 50000\n",
      "Episode 24600/30000 | Steps: 300 | Avg Reward (100ep): -11745.31 | Epsilon: 0.100 | Avg MSE Loss: 0.4670 | Buffer: 50000\n",
      "Episode 24700/30000 | Steps: 300 | Avg Reward (100ep): -11718.27 | Epsilon: 0.100 | Avg MSE Loss: 0.4550 | Buffer: 50000\n",
      "Episode 24800/30000 | Steps: 300 | Avg Reward (100ep): -11717.57 | Epsilon: 0.100 | Avg MSE Loss: 0.4561 | Buffer: 50000\n",
      "Episode 24900/30000 | Steps: 300 | Avg Reward (100ep): -11794.90 | Epsilon: 0.100 | Avg MSE Loss: 0.4621 | Buffer: 50000\n",
      "Episode 25000/30000 | Steps: 300 | Avg Reward (100ep): -11724.54 | Epsilon: 0.100 | Avg MSE Loss: 0.4643 | Buffer: 50000\n",
      "Episode 25100/30000 | Steps: 300 | Avg Reward (100ep): -11756.42 | Epsilon: 0.100 | Avg MSE Loss: 0.4632 | Buffer: 50000\n",
      "Episode 25200/30000 | Steps: 300 | Avg Reward (100ep): -11792.36 | Epsilon: 0.100 | Avg MSE Loss: 0.4728 | Buffer: 50000\n",
      "Episode 25300/30000 | Steps: 300 | Avg Reward (100ep): -11735.00 | Epsilon: 0.100 | Avg MSE Loss: 0.4699 | Buffer: 50000\n",
      "Episode 25400/30000 | Steps: 300 | Avg Reward (100ep): -11731.82 | Epsilon: 0.100 | Avg MSE Loss: 0.4635 | Buffer: 50000\n",
      "Episode 25500/30000 | Steps: 300 | Avg Reward (100ep): -11759.26 | Epsilon: 0.100 | Avg MSE Loss: 0.4591 | Buffer: 50000\n",
      "Episode 25600/30000 | Steps: 300 | Avg Reward (100ep): -11773.78 | Epsilon: 0.100 | Avg MSE Loss: 0.4538 | Buffer: 50000\n",
      "Episode 25700/30000 | Steps: 300 | Avg Reward (100ep): -11770.48 | Epsilon: 0.100 | Avg MSE Loss: 0.4589 | Buffer: 50000\n",
      "Episode 25800/30000 | Steps: 300 | Avg Reward (100ep): -11764.42 | Epsilon: 0.100 | Avg MSE Loss: 0.4628 | Buffer: 50000\n",
      "Episode 25900/30000 | Steps: 300 | Avg Reward (100ep): -11798.96 | Epsilon: 0.100 | Avg MSE Loss: 0.4664 | Buffer: 50000\n",
      "Episode 26000/30000 | Steps: 300 | Avg Reward (100ep): -11811.10 | Epsilon: 0.100 | Avg MSE Loss: 0.4722 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 26000\n",
      "======================================================================\n",
      "  Initial Q-values: [-0.53, -0.04, -0.71]\n",
      "  Total Reward: -11588.45 (-38.63 per step)\n",
      "  Actions: Nothing=0, Buy=200, Convert=100\n",
      "======================================================================\n",
      "\n",
      "Episode 26100/30000 | Steps: 300 | Avg Reward (100ep): -11731.05 | Epsilon: 0.100 | Avg MSE Loss: 0.4658 | Buffer: 50000\n",
      "Episode 26200/30000 | Steps: 300 | Avg Reward (100ep): -11702.07 | Epsilon: 0.100 | Avg MSE Loss: 0.4567 | Buffer: 50000\n",
      "Episode 26300/30000 | Steps: 300 | Avg Reward (100ep): -11777.99 | Epsilon: 0.100 | Avg MSE Loss: 0.4559 | Buffer: 50000\n",
      "Episode 26400/30000 | Steps: 300 | Avg Reward (100ep): -11759.56 | Epsilon: 0.100 | Avg MSE Loss: 0.4660 | Buffer: 50000\n",
      "Episode 26500/30000 | Steps: 300 | Avg Reward (100ep): -11716.37 | Epsilon: 0.100 | Avg MSE Loss: 0.4600 | Buffer: 50000\n",
      "Episode 26600/30000 | Steps: 300 | Avg Reward (100ep): -11779.78 | Epsilon: 0.100 | Avg MSE Loss: 0.4690 | Buffer: 50000\n",
      "Episode 26700/30000 | Steps: 300 | Avg Reward (100ep): -11703.75 | Epsilon: 0.100 | Avg MSE Loss: 0.4674 | Buffer: 50000\n",
      "Episode 26800/30000 | Steps: 300 | Avg Reward (100ep): -11712.15 | Epsilon: 0.100 | Avg MSE Loss: 0.4633 | Buffer: 50000\n",
      "Episode 26900/30000 | Steps: 300 | Avg Reward (100ep): -11778.45 | Epsilon: 0.100 | Avg MSE Loss: 0.4622 | Buffer: 50000\n",
      "Episode 27000/30000 | Steps: 300 | Avg Reward (100ep): -11726.16 | Epsilon: 0.100 | Avg MSE Loss: 0.4667 | Buffer: 50000\n",
      "Episode 27100/30000 | Steps: 300 | Avg Reward (100ep): -11731.44 | Epsilon: 0.100 | Avg MSE Loss: 0.4677 | Buffer: 50000\n",
      "Episode 27200/30000 | Steps: 300 | Avg Reward (100ep): -11733.10 | Epsilon: 0.100 | Avg MSE Loss: 0.4712 | Buffer: 50000\n",
      "Episode 27300/30000 | Steps: 300 | Avg Reward (100ep): -11753.50 | Epsilon: 0.100 | Avg MSE Loss: 0.4691 | Buffer: 50000\n",
      "Episode 27400/30000 | Steps: 300 | Avg Reward (100ep): -11770.66 | Epsilon: 0.100 | Avg MSE Loss: 0.4607 | Buffer: 50000\n",
      "Episode 27500/30000 | Steps: 300 | Avg Reward (100ep): -11758.70 | Epsilon: 0.100 | Avg MSE Loss: 0.4683 | Buffer: 50000\n",
      "Episode 27600/30000 | Steps: 300 | Avg Reward (100ep): -11755.68 | Epsilon: 0.100 | Avg MSE Loss: 0.4696 | Buffer: 50000\n",
      "Episode 27700/30000 | Steps: 300 | Avg Reward (100ep): -11754.31 | Epsilon: 0.100 | Avg MSE Loss: 0.4610 | Buffer: 50000\n",
      "Episode 27800/30000 | Steps: 300 | Avg Reward (100ep): -11772.45 | Epsilon: 0.100 | Avg MSE Loss: 0.4607 | Buffer: 50000\n",
      "Episode 27900/30000 | Steps: 300 | Avg Reward (100ep): -11681.48 | Epsilon: 0.100 | Avg MSE Loss: 0.4639 | Buffer: 50000\n",
      "Episode 28000/30000 | Steps: 300 | Avg Reward (100ep): -11795.11 | Epsilon: 0.100 | Avg MSE Loss: 0.4633 | Buffer: 50000\n",
      "\n",
      "======================================================================\n",
      "POLICY TEST AT EPISODE 28000\n",
      "======================================================================\n",
      "  Initial Q-values: [-0.57, -0.09, -0.68]\n",
      "  Total Reward: -11494.57 (-38.32 per step)\n",
      "  Actions: Nothing=0, Buy=200, Convert=100\n",
      "======================================================================\n",
      "\n",
      "Episode 28100/30000 | Steps: 300 | Avg Reward (100ep): -11742.90 | Epsilon: 0.100 | Avg MSE Loss: 0.4651 | Buffer: 50000\n",
      "Episode 28200/30000 | Steps: 300 | Avg Reward (100ep): -11735.50 | Epsilon: 0.100 | Avg MSE Loss: 0.4627 | Buffer: 50000\n",
      "Episode 28300/30000 | Steps: 300 | Avg Reward (100ep): -11732.53 | Epsilon: 0.100 | Avg MSE Loss: 0.4610 | Buffer: 50000\n",
      "Episode 28400/30000 | Steps: 300 | Avg Reward (100ep): -11785.03 | Epsilon: 0.100 | Avg MSE Loss: 0.4623 | Buffer: 50000\n",
      "Episode 28500/30000 | Steps: 300 | Avg Reward (100ep): -11777.74 | Epsilon: 0.100 | Avg MSE Loss: 0.4698 | Buffer: 50000\n",
      "Episode 28600/30000 | Steps: 300 | Avg Reward (100ep): -11764.61 | Epsilon: 0.100 | Avg MSE Loss: 0.4681 | Buffer: 50000\n",
      "Episode 28700/30000 | Steps: 300 | Avg Reward (100ep): -11744.63 | Epsilon: 0.100 | Avg MSE Loss: 0.4702 | Buffer: 50000\n",
      "Episode 28800/30000 | Steps: 300 | Avg Reward (100ep): -11764.66 | Epsilon: 0.100 | Avg MSE Loss: 0.4616 | Buffer: 50000\n",
      "Episode 28900/30000 | Steps: 300 | Avg Reward (100ep): -11802.75 | Epsilon: 0.100 | Avg MSE Loss: 0.4648 | Buffer: 50000\n",
      "Episode 29000/30000 | Steps: 300 | Avg Reward (100ep): -11752.80 | Epsilon: 0.100 | Avg MSE Loss: 0.4671 | Buffer: 50000\n",
      "Episode 29100/30000 | Steps: 300 | Avg Reward (100ep): -11743.94 | Epsilon: 0.100 | Avg MSE Loss: 0.4660 | Buffer: 50000\n",
      "Episode 29200/30000 | Steps: 300 | Avg Reward (100ep): -11800.13 | Epsilon: 0.100 | Avg MSE Loss: 0.4599 | Buffer: 50000\n",
      "Episode 29300/30000 | Steps: 300 | Avg Reward (100ep): -11793.07 | Epsilon: 0.100 | Avg MSE Loss: 0.4635 | Buffer: 50000\n",
      "Episode 29400/30000 | Steps: 300 | Avg Reward (100ep): -11670.49 | Epsilon: 0.100 | Avg MSE Loss: 0.4655 | Buffer: 50000\n",
      "Episode 29500/30000 | Steps: 300 | Avg Reward (100ep): -11769.39 | Epsilon: 0.100 | Avg MSE Loss: 0.4647 | Buffer: 50000\n",
      "Episode 29600/30000 | Steps: 300 | Avg Reward (100ep): -11745.27 | Epsilon: 0.100 | Avg MSE Loss: 0.4672 | Buffer: 50000\n",
      "Episode 29700/30000 | Steps: 300 | Avg Reward (100ep): -11753.16 | Epsilon: 0.100 | Avg MSE Loss: 0.4587 | Buffer: 50000\n",
      "Episode 29800/30000 | Steps: 300 | Avg Reward (100ep): -11722.50 | Epsilon: 0.100 | Avg MSE Loss: 0.4671 | Buffer: 50000\n",
      "Episode 29900/30000 | Steps: 300 | Avg Reward (100ep): -11728.48 | Epsilon: 0.100 | Avg MSE Loss: 0.4677 | Buffer: 50000\n",
      "\n",
      " Training Complete!\n",
      "Model saved as 'dqn_inventory_final.pth'\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-4  # DQN learning rate\n",
    "gamma = 0.95 # Gamma value\n",
    "batchSize = 64 # NN batch size\n",
    "episodes = 30000 # Max episodes\n",
    "\n",
    "# Epsilon-greedy\n",
    "epsStart = 1.0 # Initial epsilon\n",
    "epsEnd = 0.1 # Final epsilon\n",
    "epsDecayEpisodes = 20000 # Episode at which decays to final epsilon\n",
    "\n",
    "# Training controls\n",
    "startTrainAfter = 3000 # Delay to let buffer fill\n",
    "targetUpdateFreq = 1000 # Update frequency for target NN\n",
    "gradientClip = 0.5 # Used to clip gradients brought on by large rewards\n",
    "\n",
    "# Initialize\n",
    "optimizer = optim.Adam(Q_net.parameters(), lr=lr) # Adam optimizer\n",
    "lossFn = nn.MSELoss()  # MSE loss\n",
    "\n",
    "# Tracking progress\n",
    "globalSteps = 0\n",
    "episodeRewards = []\n",
    "episodeLossesHistory = []\n",
    "\n",
    "# Curriculum Learning\n",
    "def getMaxSteps(episode):\n",
    "    if episode < 8000:\n",
    "        return 50\n",
    "    elif episode < 18000:\n",
    "        return 150\n",
    "    else:\n",
    "        return 300\n",
    "\n",
    "# Train Function\n",
    "def training(Q_net, target_net, replay_buffer, optimizer, lossFn, batchSize, gamma, device):\n",
    "    states, actions, rewards, nextStates, dones = replay_buffer.sample(batchSize) # Pull samples from buffer\n",
    "\n",
    "    # DQN Inputs\n",
    "    states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    nextStates = torch.tensor(nextStates, dtype=torch.float32, device=device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "    \n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8) # Normalize rewards\n",
    "    \n",
    "    qValues = Q_net(states).gather(1, actions) # Current Q-values\n",
    "    \n",
    "    # Target Q-values\n",
    "    with torch.no_grad():\n",
    "        nextQ = target_net(nextStates).max(1)[0].unsqueeze(1)\n",
    "        qTargets = rewards + gamma * nextQ * (1 - dones)\n",
    "    \n",
    "    loss = lossFn(qValues, qTargets) # Loss function\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(Q_net.parameters(), gradientClip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "# Train Loop\n",
    "for episode in range(episodes):\n",
    "    currentMaxSteps = getMaxSteps(episode)\n",
    "    \n",
    "    # Recreate environment\n",
    "    env = InventoryManagementEnv(max_steps=currentMaxSteps)\n",
    "    env = NormalizeObservation(env)\n",
    "    \n",
    "    state, _ = env.reset() # Reset environment\n",
    "    done = False # Done flag\n",
    "    \n",
    "    # Reinitialize episode parameters\n",
    "    totalReward = 0.0\n",
    "    episodeLosses = []\n",
    "    tStep = 0\n",
    "    \n",
    "    epsilon = max(epsEnd, epsStart - (epsStart - epsEnd) * episode / epsDecayEpisodes) # Epsilon function\n",
    "\n",
    "    # Exploitation vs exploration\n",
    "    while not done and tStep < currentMaxSteps:\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                stateTensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                action = torch.argmax(Q_net(stateTensor)).item()\n",
    "        \n",
    "        nextState, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, nextState, done) # Update buffer\n",
    "\n",
    "        # Update step info\n",
    "        totalReward += reward\n",
    "        state = nextState\n",
    "        tStep += 1\n",
    "        global_steps += 1\n",
    "\n",
    "        # Fill buffer\n",
    "        if len(replay_buffer) >= startTrainAfter:\n",
    "            loss = training(Q_net, target_net, replay_buffer, optimizer, lossFn, \n",
    "                            batchSize, gamma, device)\n",
    "            episodeLosses.append(loss)\n",
    "\n",
    "        # Update target NN\n",
    "        if globalSteps % targetUpdateFreq == 0:\n",
    "            target_net.load_state_dict(Q_net.state_dict())\n",
    "            target_net.eval()\n",
    "    \n",
    "    # Update episode info\n",
    "    episodeRewards.append(totalReward)\n",
    "    avgLoss = np.mean(episodeLosses) if episodeLosses else 0.0\n",
    "    episodeLossesHistory.append(avgLoss)\n",
    "    \n",
    "    # Logging\n",
    "    if episode % 100 == 0:\n",
    "        avgReward100 = np.mean(episodeRewards[-100:]) if len(episodeRewards) >= 100 else np.mean(episodeRewards)\n",
    "        avgLoss100 = np.mean(episodeLossesHistory[-100:]) if len(episodeLossesHistory) >= 100 else avgLoss\n",
    "        print(f\"Episode {episode}/{episodes} | Steps: {currentMaxSteps} | \"\n",
    "              f\"Avg Reward (100ep): {avgReward100:.2f} | \"\n",
    "              f\"Epsilon: {epsilon:.3f} | Avg MSE Loss: {avgLoss100:.4f} | Buffer: {len(replay_buffer)}\")\n",
    "\n",
    "print(\"\\n Training Complete!\")\n",
    "torch.save(Q_net.state_dict(), 'dqn_inventory_final.pth')\n",
    "print(\"Model saved as 'dqn_inventory_final.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.74, Product Price: 19.98\n",
      "Demand: 13.01, Cash: 1000.00\n",
      "Step: 1\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.85, Product Price: 19.73\n",
      "Demand: 14.02, Cash: 995.26\n",
      "Step: 2\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.76, Product Price: 20.03\n",
      "Demand: 8.49, Cash: 990.41\n",
      "Step: 3\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 4.74, Product Price: 19.89\n",
      "Demand: 8.44, Cash: 1010.44\n",
      "Step: 4\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.05, Product Price: 19.99\n",
      "Demand: 12.62, Cash: 1005.70\n",
      "Step: 5\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.07, Product Price: 20.24\n",
      "Demand: 11.54, Cash: 1000.65\n",
      "Step: 6\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.43, Product Price: 19.95\n",
      "Demand: 11.25, Cash: 1020.89\n",
      "Step: 7\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.48, Product Price: 19.82\n",
      "Demand: 11.99, Cash: 1015.46\n",
      "Step: 8\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.20, Product Price: 19.98\n",
      "Demand: 12.43, Cash: 1009.98\n",
      "Step: 9\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.11, Product Price: 19.54\n",
      "Demand: 10.05, Cash: 1029.95\n",
      "Step: 10\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.29, Product Price: 19.53\n",
      "Demand: 9.73, Cash: 1024.84\n",
      "Step: 11\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.21, Product Price: 19.93\n",
      "Demand: 7.85, Cash: 1019.56\n",
      "Step: 12\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.28, Product Price: 19.95\n",
      "Demand: 7.72, Cash: 1039.49\n",
      "Step: 13\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.52, Product Price: 19.75\n",
      "Demand: 8.31, Cash: 1034.21\n",
      "Step: 14\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.80, Product Price: 19.73\n",
      "Demand: 6.88, Cash: 1028.69\n",
      "Step: 15\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.81, Product Price: 19.81\n",
      "Demand: 9.21, Cash: 1048.42\n",
      "Step: 16\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.85, Product Price: 19.94\n",
      "Demand: 14.27, Cash: 1042.61\n",
      "Step: 17\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.02, Product Price: 20.09\n",
      "Demand: 13.89, Cash: 1036.76\n",
      "Step: 18\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 6.30, Product Price: 19.47\n",
      "Demand: 12.74, Cash: 1056.85\n",
      "Step: 19\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.38, Product Price: 19.78\n",
      "Demand: 12.28, Cash: 1050.55\n",
      "Step: 20\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.31, Product Price: 19.83\n",
      "Demand: 10.18, Cash: 1044.17\n",
      "Step: 21\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 6.34, Product Price: 19.98\n",
      "Demand: 7.22, Cash: 1064.00\n",
      "Step: 22\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.07, Product Price: 20.03\n",
      "Demand: 5.43, Cash: 1057.66\n",
      "Step: 23\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.01, Product Price: 20.14\n",
      "Demand: 9.70, Cash: 1051.59\n",
      "Step: 24\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.92, Product Price: 20.22\n",
      "Demand: 13.91, Cash: 1071.73\n",
      "Step: 25\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.99, Product Price: 20.25\n",
      "Demand: 11.15, Cash: 1065.81\n",
      "Step: 26\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.01, Product Price: 20.02\n",
      "Demand: 10.80, Cash: 1059.82\n",
      "Step: 27\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.90, Product Price: 20.32\n",
      "Demand: 8.67, Cash: 1079.84\n",
      "Step: 28\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.23, Product Price: 20.40\n",
      "Demand: 10.37, Cash: 1073.94\n",
      "Step: 29\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.08, Product Price: 20.65\n",
      "Demand: 11.32, Cash: 1067.71\n",
      "Step: 30\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 6.00, Product Price: 20.63\n",
      "Demand: 10.69, Cash: 1088.36\n",
      "Step: 31\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.04, Product Price: 20.82\n",
      "Demand: 10.16, Cash: 1082.35\n",
      "Step: 32\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 6.18, Product Price: 20.86\n",
      "Demand: 6.37, Cash: 1076.31\n",
      "Step: 33\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 6.21, Product Price: 20.43\n",
      "Demand: 7.89, Cash: 1097.17\n",
      "Step: 34\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.91, Product Price: 20.18\n",
      "Demand: 8.11, Cash: 1090.96\n",
      "Step: 35\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.66, Product Price: 19.84\n",
      "Demand: 13.60, Cash: 1085.05\n",
      "Step: 36\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.52, Product Price: 19.84\n",
      "Demand: 10.40, Cash: 1104.89\n",
      "Step: 37\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.63, Product Price: 19.64\n",
      "Demand: 12.53, Cash: 1099.37\n",
      "Step: 38\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.48, Product Price: 19.28\n",
      "Demand: 11.44, Cash: 1093.74\n",
      "Step: 39\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.77, Product Price: 19.40\n",
      "Demand: 6.39, Cash: 1113.01\n",
      "Step: 40\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.67, Product Price: 19.33\n",
      "Demand: 8.17, Cash: 1107.24\n",
      "Step: 41\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.52, Product Price: 19.08\n",
      "Demand: 8.40, Cash: 1101.57\n",
      "Step: 42\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.27, Product Price: 19.35\n",
      "Demand: 10.28, Cash: 1120.65\n",
      "Step: 43\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.11, Product Price: 19.49\n",
      "Demand: 10.66, Cash: 1115.38\n",
      "Step: 44\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.98, Product Price: 19.85\n",
      "Demand: 12.59, Cash: 1110.27\n",
      "Step: 45\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.28, Product Price: 19.69\n",
      "Demand: 14.78, Cash: 1130.12\n",
      "Step: 46\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.13, Product Price: 19.71\n",
      "Demand: 11.10, Cash: 1124.84\n",
      "Step: 47\n",
      "Raw Inventory: 2.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 5.00, Product Price: 19.43\n",
      "Demand: 8.61, Cash: 1119.71\n",
      "Step: 48\n",
      "Raw Inventory: 0.0\n",
      "Product Inventory Before Sale: 1.0, After Sale: 0.0\n",
      "Raw Price: 5.13, Product Price: 19.20\n",
      "Demand: 12.55, Cash: 1139.14\n",
      "Step: 49\n",
      "Raw Inventory: 1.0\n",
      "Product Inventory Before Sale: 0.0, After Sale: 0.0\n",
      "Raw Price: 4.93, Product Price: 19.13\n",
      "Demand: 13.67, Cash: 1134.01\n",
      "Environment ended early at step 50.\n"
     ]
    }
   ],
   "source": [
    "# Test Policy\n",
    "Q_net.load_state_dict(torch.load(\"dqn_inventory_final.pth\", map_location=device))\n",
    "Q_net.eval()\n",
    "\n",
    "state, info = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(50):\n",
    "    env.render()\n",
    "\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(Q_net(state_tensor)).item()\n",
    "\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "    state = next_state\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Environment ended early at step {step+1}.\")\n",
    "        break\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One of the primary challenges I encountered was managing the large negative reward values caused by stockout penalties. These penalties not only caused instability in the DQN, often leading to exploding gradients, but also shifted the agents focus almost entirely toward avoiding stockouts, at the expense of learning other beneficial actions. To address this, I applied gradient clipping and reward normalization to keep training stable and to ensure that the reward signal remained informative. During troubleshooting, I also implemented extensive logging to monitor the models behavior throughout training. This experience emphasized the importance of observing the neural networks loss trends, as waiting solely for increases in cumulative reward is impractical for slow converging models. One question I still have is how to determine whether the model has converged to a local minimum rather than the optimal policy. In a real-world deployment, would it be advantageous to maintain a prolonged exploration period to ensure sufficient coverage of the state space?\n",
    "2. Overall, the model performed reasonably well, achieving approximately a 10% increase in cash over 50 steps.\n",
    "3. Reinforcement learning offers several advantages over traditional control methods, and control plays a central role across many industries. In agriculture in particular, RL has the potential to significantly improve system performance. Biological processes are often highly non-linear and influenced by numerous interacting variables, which makes them difficult to manage using traditional PID controllers that rely on linear assumptions. RL, by contrast, is inherently well-suited for non-linear and dynamic environments, enabling it to adapt control policies based on feedback and changing conditions. Furthermore, within the broader context of AI development, RL is considered a key component in progressing toward artificial general intelligence (AGI). Unlike large language models, which are stateless and limited to the distribution of data on which they were trained, RL agents actively interact with and explore their environments. This ability allows RL systems to discover new states and behaviors beyond their initial training data, and often at a lower computational cost than retraining or scaling static models. As a result, RL represents a promising path toward building systems that can operate flexibly in open-ended, real-world environments.\n",
    "\n",
    "Note: I showcased the model running for 50 steps rather than 10 to better show the increase in cash.\n",
    "\n",
    "ChatGPT Help:\n",
    "Prompt: \"Adjust for grammar, spelling, and academic tone\"\n",
    "Result: See above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
